{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9196d582",
   "metadata": {},
   "source": [
    "# üéì UQ Course Scraper & Data Engineer\n",
    "**Project:** UQ Course Navigator & Grade Tracker  \n",
    "**Phase:** 1 - Data Engineering  \n",
    "**Status:** ‚úÖ Ready for Phase 2 (Database)\n",
    "\n",
    "---\n",
    "**M·ª•c ti√™u:**\n",
    "1. Thu th·∫≠p th√¥ng tin chi ti·∫øt c√°c m√¥n h·ªçc (Description, Units, Contact Hours).\n",
    "2. Truy c·∫≠p Electronic Course Profile (ECP) ƒë·ªÉ l·∫•y b·∫£ng ƒëi·ªÉm (Assessments).\n",
    "3. X·ª≠ l√Ω d·ªØ li·ªáu th√¥: G·∫Øn c·ªù (Hurdle, Team-based), t√≠nh tr·ªçng s·ªë %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3ed18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import pandas as pd # D√πng pandas ƒë·ªÉ xem b·∫£ng cho ƒë·∫πp trong Notebook\n",
    "from tqdm import tqdm # <--- TH√äM D√íNG N√ÄY\n",
    "import time # <--- TH√äM D√íNG N√ÄY (ƒë·ªÉ d√πng sleep tr√°nh b·ªã ch·∫∑n IP)\n",
    "import concurrent.futures\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2cecc",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 1. Core Scraper Functions\n",
    "Ph·∫ßn n√†y ƒë·ªãnh nghƒ©a c√°c h√†m x·ª≠ l√Ω ch√≠nh:\n",
    "* **`scrape_uq_course`**: L·∫•y th√¥ng tin t·ªïng quan t·ª´ trang ch·ªß m√¥n h·ªçc.\n",
    "* **`scrape_assessment_table`**: ƒêi s√¢u v√†o link ECP ƒë·ªÉ b√≥c t√°ch b·∫£ng ƒëi·ªÉm.\n",
    "* **`clean_assessment_task`**: D√πng Regex ƒë·ªÉ l√†m s·∫°ch t√™n b√†i t·∫≠p v√† g·∫Øn c·ªù (`is_hurdle`, `is_in_person`).\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985a163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_course_codes(text):\n",
    "    return re.findall(r'[A-Z]{4}\\d{4}', text)\n",
    "\n",
    "def scrape_uq_course(course_code):\n",
    "    url = f\"https://my.uq.edu.au/programs-courses/course.html?course_code={course_code}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Helper function ƒë·ªÉ l·∫•y text an to√†n\n",
    "        def get_text(selector_id):\n",
    "            element = soup.find(id=selector_id)\n",
    "            return element.get_text(strip=True) if element else \"N/A\"\n",
    "\n",
    "        # 1. Th√¥ng tin ƒë·ªãnh danh\n",
    "        full_title = get_text('course-title')\n",
    "        # T√°ch l·∫•y t√™n m√¥n (b·ªè ph·∫ßn m√£ m√¥n trong ngo·∫∑c)\n",
    "        course_name = re.sub(r'\\s\\([A-Z]{4}\\d{4}\\)', '', full_title)\n",
    "\n",
    "        # 2. Th√¥ng tin chi ti·∫øt (Summary Panel)\n",
    "        level = get_text('course-level')\n",
    "        faculty = get_text('course-faculty')\n",
    "        school = get_text('course-school')\n",
    "        units = int(get_text('course-units'))\n",
    "        duration = get_text('course-duration')\n",
    "        mode = get_text('course-mode')\n",
    "        contact_hours = soup.find(id='course-contact').get_text(separator=' ', strip=True) if soup.find(id='course-contact') else \"N/A\"        \n",
    "        \n",
    "        # 3. ƒêi·ªÅu ki·ªán v√† R√†ng bu·ªôc\n",
    "        prereq_raw = get_text('course-prerequisite')\n",
    "        incomp_raw = get_text('course-incompatible')\n",
    "        \n",
    "        # 4. T√≥m t·∫Øt n·ªôi dung & ƒê√°nh gi√° s∆° b·ªô\n",
    "        description = get_text('course-summary')\n",
    "        assessment_summary = get_text('course-assessment-methods')\n",
    "        coordinator = get_text('course-coordinator')\n",
    "\n",
    "        # 5. Link quan tr·ªçng\n",
    "        ecp_link = \"\"\n",
    "        ecp_tag = soup.find('a', class_='profile-available')\n",
    "        if ecp_tag:\n",
    "            ecp_link = ecp_tag['href']\n",
    "            # N·∫øu link l√† t∆∞∆°ng ƒë·ªëi, n·ªëi th√™m domain\n",
    "            if ecp_link.startswith('/'):\n",
    "                ecp_link = \"https://programs-courses.uq.edu.au\" + ecp_link\n",
    "\n",
    "        return {\n",
    "            \"code\": course_code,\n",
    "            \"title\": course_name,\n",
    "            \"units\": units,\n",
    "            \"level\": level,\n",
    "            \"faculty\": faculty,\n",
    "            \"school\": school,\n",
    "            \"description\": description,\n",
    "            \"contact_hours\": contact_hours,\n",
    "            \"assessment_summary\": assessment_summary,\n",
    "            \"prerequisites_text\": prereq_raw,\n",
    "            \"prerequisites_list\": extract_course_codes(prereq_raw),\n",
    "            \"incompatible_list\": extract_course_codes(incomp_raw),\n",
    "            \"coordinator\": coordinator,\n",
    "            \"ecp_link\": ecp_link,\n",
    "            \"url\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {course_code}: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_assessment_task(raw_name):\n",
    "    # Kh·ªüi t·∫°o c√°c flag m·∫∑c ƒë·ªãnh l√† False\n",
    "    flags = {\n",
    "        \"is_hurdle\": False,\n",
    "        \"is_identity_verified\": False,\n",
    "        \"is_in_person\": False,\n",
    "        \"is_team_based\": False\n",
    "    }\n",
    "    \n",
    "    # 1. Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c√°c t·ª´ kh√≥a (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)\n",
    "    if re.search(r'hurdle', raw_name, re.IGNORECASE):\n",
    "        flags[\"is_hurdle\"] = True\n",
    "    if re.search(r'identity verified', raw_name, re.IGNORECASE):\n",
    "        flags[\"is_identity_verified\"] = True\n",
    "    if re.search(r'in-person', raw_name, re.IGNORECASE):\n",
    "        flags[\"is_in_person\"] = True\n",
    "    if re.search(r'team', raw_name, re.IGNORECASE):\n",
    "        flags[\"is_team_based\"] = True\n",
    "        \n",
    "    # 2. X√≥a c√°c t·ª´ kh√≥a n√†y kh·ªèi chu·ªói\n",
    "    # Regex n√†y t√¨m c√°c t·ª´ ƒë√≥ k√®m theo d·∫•u ph·∫©y ho·∫∑c ngo·∫∑c ƒë∆°n xung quanh ch√∫ng\n",
    "    clean_name = re.sub(r'\\(?Hurdle\\)?', '', raw_name, flags=re.IGNORECASE)\n",
    "    clean_name = re.sub(r'\\(?Identity Verified\\)?', '', clean_name, flags=re.IGNORECASE)\n",
    "    clean_name = re.sub(r'\\(?In-person\\)?', '', clean_name, flags=re.IGNORECASE)\n",
    "    clean_name = re.sub(r'\\(?Team or group-based\\)?', '', clean_name, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 3. D·ªçn d·∫πp c√°c k√Ω t·ª± th·ª´a (d·∫•u ph·∫©y d∆∞, kho·∫£ng tr·∫Øng d∆∞)\n",
    "    clean_name = clean_name.replace(', ,', ',').strip(' ,()')\n",
    "    clean_name = re.sub(r'\\s+', ' ', clean_name) # X√≥a kho·∫£ng tr·∫Øng k√©p\n",
    "    \n",
    "    return clean_name, flags\n",
    "\n",
    "def scrape_assessment_table(ecp_url):\n",
    "    if not ecp_url or ecp_url == \"N/A\":\n",
    "        return []\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o ch√∫ng ta v√†o ƒë√∫ng trang Assessment (Section 5)\n",
    "    # L∆∞u √Ω: T√πy link m√† UQ cung c·∫•p, c√≥ khi ph·∫£i append th√™m ƒë·ªÉ ra trang full assessment\n",
    "    # ·ªû phi√™n b·∫£n V1, ta gi·∫£ s·ª≠ link d·∫´n ƒë·∫øn trang c√≥ ch·ª©a b·∫£ng Assessment\n",
    "    \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        response = requests.get(ecp_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        assessments = []\n",
    "        \n",
    "        # T√¨m b·∫£ng Assessment - UQ th∆∞·ªùng d√πng class 'assessment-details' ho·∫∑c t√¨m theo text 'Assessment Task'\n",
    "        table = soup.find('section', class_='section section--course-profile section--in-view') \n",
    "        \n",
    "        if not table:\n",
    "            # Plan B: T√¨m table b·∫•t k·ª≥ c√≥ ch·ª©a ch·ªØ \"Weight\"\n",
    "            tables = soup.find_all('table')\n",
    "            for t in tables:\n",
    "                if \"Weight\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if table:\n",
    "            rows = table.find_all('tr')[1:] # B·ªè qua h√†ng ti√™u ƒë·ªÅ (header)\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                if len(cols) >= 2:\n",
    "                    category = cols[0].get_text(strip=True)\n",
    "                    assesment_task= cols[1].get_text(strip=True)\n",
    "                    \n",
    "                    \n",
    "                    weight_raw = cols[2].get_text(strip=True)\n",
    "                    due_date = cols[3].get_text(separator=' ', strip=True) if len(cols) > 3 else \"N/A\"                    \n",
    "                    \n",
    "                    # Clean tr·ªçng s·ªë: \"20%\" -> 0.2\n",
    "                    weight_percent = re.findall(r'\\d+', weight_raw)\n",
    "                    weight_value = int(weight_percent[0]) / 100 if weight_percent else 0\n",
    "                    \n",
    "                    # Trong h√†m scrape_assessment_table, ƒëo·∫°n x·ª≠ l√Ω col[0] (task_name):\n",
    "\n",
    "                    task_name_raw = cols[1].get_text(strip=True)\n",
    "                    clean_name, flags = clean_assessment_task(task_name_raw)\n",
    "\n",
    "                    assessments.append({\n",
    "                        \"category\": category,\n",
    "                        \"assesment_task\": clean_name,\n",
    "                        \"weight\": weight_value,\n",
    "                        \"due_date\": due_date,\n",
    "                        \"flags\": flags\n",
    "                        \n",
    "                    })\n",
    "        \n",
    "        return assessments\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi c√†o b·∫£ng ƒëi·ªÉm t·∫°i {ecp_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- H√ÄM T·ªîNG H·ª¢P ---\n",
    "def get_full_course_data(course_code):\n",
    "    course_code = course_code.upper()\n",
    "    # B∆∞·ªõc 1: L·∫•y th√¥ng tin chung\n",
    "    course_data = scrape_uq_course(course_code)\n",
    "    \n",
    "    if course_data and course_data['ecp_link']:\n",
    "        course_data['assessments'] = scrape_assessment_table(course_data['ecp_link'])\n",
    "        \n",
    "    return course_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31832cf",
   "metadata": {},
   "source": [
    "## üì• 2. Load Input Data\n",
    "ƒê·ªçc danh s√°ch m√£ m√¥n h·ªçc (Course Codes) t·ª´ file `data/eait_codes_only.json` ƒë√£ ƒë∆∞·ª£c qu√©t ·ªü b∆∞·ªõc tr∆∞·ªõc (Scan Phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d222dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t√¨m th·∫•y file t·∫°i: ../data/all_course_codes.json\n",
      "‚úÖ ƒê√£ load th√†nh c√¥ng 3860 m√£ m√¥n.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---\n",
    "# ƒêi t·ª´ folder 'scraper' l√πi ra ngo√†i (..) r·ªìi v√†o folder 'data'\n",
    "input_path = os.path.join('..', 'data', 'all_course_codes.json')\n",
    "\n",
    "# ƒê·ªçc file danh s√°ch m√£ m√¥n\n",
    "try:\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        course_list = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ t√¨m th·∫•y file t·∫°i: {input_path}\")\n",
    "    print(f\"‚úÖ ƒê√£ load th√†nh c√¥ng {len(course_list)} m√£ m√¥n.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i '{input_path}'\")\n",
    "    print(\"üëâ H√£y ki·ªÉm tra l·∫°i xem file json ƒë√£ n·∫±m trong folder 'data' ch∆∞a.\")\n",
    "    course_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a472fbb",
   "metadata": {},
   "source": [
    "## üöÄ 3. Main Execution Loop\n",
    "B·∫Øt ƒë·∫ßu qu√° tr√¨nh c√†o d·ªØ li·ªáu chi ti·∫øt.\n",
    "> **L∆∞u √Ω:**\n",
    "> * S·ª≠ d·ª•ng `tqdm` ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn ƒë·ªô.\n",
    "> * Script s·∫Ω t·∫°m ngh·ªâ `0.5s` gi·ªØa m·ªói request ƒë·ªÉ tu√¢n th·ªß quy t·∫Øc Rate Limiting c·ªßa UQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd3e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu v·ªõi 20 lu·ªìng song song...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3860/3860 [11:52<00:00,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH T·ªêC ƒê·ªò ---\n",
    "MAX_WORKERS = 20  # S·ªë lu·ªìng ch·∫°y song song (ƒê·ª´ng ƒë·ªÉ qu√° cao, UQ s·∫Ω ch·∫∑n. 5-10 l√† an to√†n)\n",
    "\n",
    "results = []\n",
    "failed_courses = []\n",
    "\n",
    "print(f\"üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu v·ªõi {MAX_WORKERS} lu·ªìng song song...\")\n",
    "\n",
    "\n",
    "# H√†m wrapper ƒë·ªÉ x·ª≠ l√Ω ngo·∫°i l·ªá trong Thread\n",
    "def process_course(code):\n",
    "    try:\n",
    "        # Kh√¥ng c·∫ßn time.sleep ·ªü ƒë√¢y n·ªØa v√¨ m·∫°ng s·∫Ω t·ª± t·∫°o ƒë·ªô tr·ªÖ t·ª± nhi√™n\n",
    "        # ho·∫∑c gi·ªØ time.sleep(0.1) n·∫øu mu·ªën c·ª±c k·ª≥ an to√†n\n",
    "        return get_full_course_data(code)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# S·ª¨ D·ª§NG THREAD POOL\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # G·ª≠i t·∫•t c·∫£ nhi·ªám v·ª• v√†o Pool\n",
    "    future_to_code = {executor.submit(process_course, code): code for code in course_list}\n",
    "    \n",
    "    # D√πng tqdm ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô khi c√°c task ho√†n th√†nh\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_code), total=len(course_list), desc=\"Downloading\"):\n",
    "        code = future_to_code[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "            if data:\n",
    "                results.append(data)\n",
    "            else:\n",
    "                failed_courses.append(code)\n",
    "        except Exception as exc:\n",
    "            print(f\"‚ö†Ô∏è {code} sinh ra l·ªói: {exc}\")\n",
    "            failed_courses.append(code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86878bb",
   "metadata": {},
   "source": [
    "## üìä 4. Data Verification & Export\n",
    "* Ki·ªÉm tra nhanh d·ªØ li·ªáu b·∫±ng `pandas`.\n",
    "* Xu·∫•t to√†n b·ªô d·ªØ li·ªáu s·∫°ch ra file `data/master_courses.json` ƒë·ªÉ chu·∫©n b·ªã import v√†o Supabase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5bb8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ho√†n t·∫•t! ƒê√£ c√†o 3860 m√¥n. (Th·∫•t b·∫°i: 0)\n",
      "‚úÖ ƒê√£ l∆∞u file th√†nh c√¥ng t·∫°i: ../data/master_courses.json\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "output_path = os.path.join('..', 'data', 'master_courses.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"‚úÖ Ho√†n t·∫•t! ƒê√£ c√†o {len(results)} m√¥n. (Th·∫•t b·∫°i: {len(failed_courses)})\")\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u file th√†nh c√¥ng t·∫°i: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
